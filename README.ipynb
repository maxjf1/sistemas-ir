{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho 3: Avaliando sistemas de IR\n",
    "\n",
    "Neste trabalho, o aluno irá explorar o problema de otimização de sistemas de IR através da análise de métricas de avaliação populares para esse tipo de sistema.\n",
    "\n",
    "Para isso, vamos usar a base CFC (com artigos sobre fibrose cística), a qual encontra-se nesta pasta.\n",
    "\n",
    "Observação: a idéia aqui é usar o jupyter para mostrar a evolução dos teus experimentos. Então use ele adequadamente. Ou seja, não altere funções do começo para reexecutar algo que foi feito lá embaixo. Neste caso, sobreescreva a função lá embaixo. A leitura do notebook tem que ser feita sequencialmente para ficar fácil, ok?\n",
    "\n",
    "\n",
    "## Passo 1 - Normalização\n",
    "\n",
    "Primeira coisa que você vai precisar fazer é ler os arquivos do CFC. Essa é uma parte meio braçal mesmo, mas não tem como escapar dela. Então você terá que criar funções que consigam ler cada arquivo do CFC e parsear eles.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 27,
>>>>>>> 99cc5444df6f84b339de190b5321436c3d172878
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extractCFCArticles(path):\n",
    "    # Lista de campos do arquivo CFC\n",
    "    fields = [\n",
    "            'PN',\n",
    "            'RN',\n",
    "            'AN',\n",
    "            'AU',\n",
    "            'TI',\n",
    "            'SO',\n",
    "            'MJ',\n",
    "            'MN',\n",
    "            'AB',\n",
    "            'EX',\n",
    "            'RF',\n",
    "            'CT',\n",
    "    ]\n",
    "    \n",
    "    # Lista de campos que mudarao seus nomes\n",
    "    cast = {\n",
    "        'RN': 'record',\n",
    "        'AB': 'content',\n",
    "        'EX': 'content',\n",
    "        'AN': 'an',\n",
    "        'TI': 'title'\n",
    "    }\n",
    "    \n",
    "    file = open(path).read()\n",
    "    articles = []\n",
    "\n",
    "    for article in file.split(\"\\n\\n\"):\n",
    "        # Objeto do artigo\n",
    "        articleData = {\n",
    "            'title':'',\n",
    "            'record': 0,\n",
    "            'year': 0,\n",
    "            'number': 0,\n",
    "            'an': 0,\n",
    "            'AU': '',\n",
    "            'SO': '',\n",
    "            'MJ': [],\n",
    "            'MN': [],\n",
    "            'RF': [],\n",
    "            'CT': [],\n",
    "            'contentType': '',\n",
    "            'content': '',\n",
    "        }\n",
    "        currentField = ''\n",
    "        for line in article.split(\"\\n\"):\n",
    "            # Linha vazia ou fim de arquivo\n",
    "            if(not line.strip() or line.startswith(\"\\x1a\")):\n",
    "                continue \n",
    "            \n",
    "            for key in fields:\n",
    "                # Verificase a linha é a definição de um novo atributo\n",
    "                if(line.startswith(key + '')):\n",
    "                    currentField = line[0:2].upper()\n",
    "                    line = line[3:]\n",
    "            line = line.strip()\n",
    "            if(currentField == 'RF' or currentField == 'CT'):\n",
    "                articleData[currentField].append(line)\n",
    "                continue\n",
    "                \n",
    "            if(currentField == 'MJ' or currentField == 'MN'):\n",
    "                articleData[currentField]+= line.split(\"  \")\n",
    "                continue\n",
    "            \n",
    "            if(currentField == 'PN'):\n",
    "                articleData['year'] = int(line[0:2])\n",
    "                articleData['number'] = int(line[2])\n",
    "                continue\n",
    "            \n",
    "            if(currentField == 'AB' or currentField == 'EX'):\n",
    "                articleData['contentType'] = currentField\n",
    "                \n",
    "            if(currentField in cast):\n",
    "                currentField = cast[currentField]\n",
    "            \n",
    "            if(currentField == 'record' or currentField == 'an'):\n",
    "                articleData[currentField] = int(line)\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            if(articleData[currentField]):\n",
    "                articleData[currentField]+=' '\n",
    "            articleData[currentField]+= line \n",
    "        \n",
    "        if(articleData['title']):\n",
    "            articles.append(articleData)\n",
    "    return articles\n",
    "            \n",
    "\n",
    "articles = [\n",
    "    'data/cf74',\n",
    "    'data/cf75',\n",
    "    'data/cf76',\n",
    "    'data/cf77',\n",
    "    'data/cf78',\n",
    "    'data/cf79'\n",
    "]\n",
    "\n",
    "import json\n",
    "data = extractCFCArticles(articles[5])\n",
    "\n",
    "# print(json.dumps(data[0], indent=4))\n",
    "\n",
    "# pd.DataFrame(data).to_json(orient='table')\n",
    "# for i in data:\n",
    "#     print(i['AB'] and i['EX'])\n",
    "#     print(i, \"\\n--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 2 - Primeira indexação\n",
    "\n",
    "Depois de parsear os arquivos, agora é hora de indexar os dados. Ao invés de criar o código do zero, vamos usar o Whoosh, que é uma implementação em python inspirada no Lucene. A documentação pode ser encontrada em https://whoosh.readthedocs.io/en/latest/index.html.\n",
    "\n",
    "Nesta questão de indexação, temos que fazer algumas escolhas: \n",
    "* Como fazer o processo de tokenização? \n",
    "* Stemmizar ou não stemmizar, eis a questão...\n",
    "* Quais campos são úteis para indexação?\n",
    "\n",
    "Neste momento, você vai tomar suas decisões iniciais. A ideia é você testar, depois, outras soluções para verificar quais tiveram os melhores resultados, entendeu? Então não esqueça de documentar, aqui, qual a sua decisão inicial e depois ir explicando ao longo do notebook os experimentos que está fazendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para pesquisa e indexação, iremos avaliar principalmente os subjects. Estes, levantados por especialistas, contém informações sobre palavras-chave que definem os artigos. \n",
    "\n",
    "No geral, cada artigo possui um Major e Minor Subject. O primeiro define o foco do artigo enquanto o segundo, temas secundários abordados.\n",
    "\n",
    "Dentro dos subjects, os termos são definidos seguindo o [vocabulário médico MeSH](https://meshb.nlm.nih.gov/), em alguns casos seguido de uma sigla de qualificadores (Ex di:diagnóstico).\n",
    "\n",
    "A decisão inicial é usar destas informações para gerar indexação e ranqueamento de pesquisas. Priorizando o Major subject junto do qualificador, em seguida minor subjects\n",
    "\n",
    "Usaremos também os autores e citações para calcular o H-index dos artigos"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Schema: ['content']>\n"
=======
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whoosh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-98796f93660e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwhoosh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwhoosh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m schema = Schema(title=TEXT(stored=True), \n\u001b[1;32m      5\u001b[0m                 \u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstored\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whoosh'"
>>>>>>> 99cc5444df6f84b339de190b5321436c3d172878
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#códigos...\n",
    "\n",
    "from whoosh.analysis import CharsetFilter, StemmingAnalyzer\n",
    "from whoosh import fields\n",
    "from whoosh.support.charset import accent_map\n",
    "\n",
    "# For example, to add an accent-folding filter to a stemming analyzer:\n",
    "my_analyzer = StemmingAnalyzer() | CharsetFilter(data)\n",
    "\n",
    "# To use this analyzer in your schema:\n",
    "my_schema = fields.Schema(content=fields.TEXT(analyzer=my_analyzer))\n",
    "print(my_schema)"
=======
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "\n",
    "schema = Schema(title=TEXT(stored=True), \n",
    "                record=ID(stored=True), \n",
    "                year=NUMERIC(stored=True),\n",
    "                number=NUMERIC(stored=True),\n",
    "                an=NUMERIC(stored=True),\n",
    "                content=TEXT)\n",
    "\n",
    "ix = create_in(\"indexdir\", schema)\n",
    "writer = ix.writer()\n",
    "\n",
    "for article in extractCFCArticles(articles[1]):\n",
    "    writer.add_document(title=article['title'],\n",
    "                        record=article['record'],\n",
    "                        year=article['year'],\n",
    "                        number=article['number'],\n",
    "                        an=article['an'],\n",
    "                        content=article['content'],\n",
    "                       )\n",
    "\n",
    "writer.add_document(title=u\"First document\", path=u\"/a\", content=u\"This is the first document we've added!\")\n",
    "writer.commit()\n",
    "\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "with ix.searcher() as searcher:\n",
    "     query = QueryParser(\"content\", ix.schema).parse(\"first\")\n",
    "     results = searcher.search(query)\n",
    "     results[0]"
>>>>>>> 99cc5444df6f84b339de190b5321436c3d172878
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3 - Calculando as métricas de avaliação\n",
    "\n",
    "Para avaliar a tua decisão, vamos usar como métricas a precisão, recall e f-measure. Faça uma variação da precisão e recall como P@n e R@n, pois serão úteis para os gráficos que serão gerados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#códigos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 4 - Gráficos do experimento 1\n",
    "\n",
    "Apresentar agora os gráficos do teu experimento. Você deve gerar um gráfico PxR, como visto em sala. Para gerar gráficos, você pode usar o matplotlib. Se quiser usar outra biblioteca que ache mais fácil, sem problemas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# códigos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 5 - Demais experimentos\n",
    "\n",
    "Agora você irá testar novas configurações de campos e configurações para tentar encontrar qual a que te dá melhores resultados. Mostrar, como um relatório, a evolução do trabalho. Ou seja, mostrar como o trabalho foi evoluindo para você alcançar o melhor resultado (quais modificações foram feitas, como cada modificação influenciou nas métricas, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# códigos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 6 - Quantos resultados eu devo voltar para o usuário?\n",
    "\n",
    "Um dos problemas de sistemas de IR é determinar quantos resultados você deve retornar ao usuário. Uma forma de entender o comportamento do sistema e qual o corte ideal na lista resultante é através do uso de curvas ROC. Assim, plote a curva ROC do teu sistema e determine, através da análise da curva, qual o ponto ideal de corte para o teu sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# códigos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
